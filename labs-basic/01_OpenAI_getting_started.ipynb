{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# OpenAI Quickstart"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Overview  \n",
        "\"Large language models are functions that map text to text. Given an input string of text, a large language model tries to predict the text that will come next\"(1). This \"quickstart\" notebook will introduce users to high-level LLM concepts, core package requirements for getting started with AML, an soft introduction to prompt design, and several short examples of different use cases.  \n",
        "\n",
        "For more quickstart examples please refer to the official Azure Open AI Quickstart Documentation https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quickstart?pivots=programming-language-studio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table of Contents  \n",
        "\n",
        "[Overview](#overview)  \n",
        "[How to use OpenAI Service](#how-to-use-openai-service)  \n",
        "[1. Creating your OpenAI Service](#1.-creating-your-openai-service)  \n",
        "[2. Installation](#2.-installation)    \n",
        "[3. Credentials](#3.-credentials)  \n",
        "\n",
        "[Use Cases](#use-cases)    \n",
        "[1. Summarize Text](#1.-summarize-text)  \n",
        "[2. Classify Text](#2.-classify-text)  \n",
        "[3. Generate New Product Names](#3.-generate-new-product-names)  \n",
        "[4. Fine Tune a Classifier](#4.fine-tune-a-classifier)  \n",
        "[5. Embeddings!]((#5.-embeddings!))\n",
        "\n",
        "[References](#references)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Getting started with Azure OpenAI Service\n",
        "\n",
        "Customers can log into the Azure portal, create an Azure OpenAI Service resource, and start experimenting with models via the studio  \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Build your first prompt  \n",
        "This short exercise will provide a basic introduction for submitting prompts to an OpenAI model for a simple task \"summarization\".  \n",
        "\n",
        "![](images/generative-AI-models-reduced.jpg)  \n",
        "\n",
        "\n",
        "**Steps**:  \n",
        "1. Install OpenAI library in your python environment  \n",
        "2. Load standard helper libraries and set your typical OpenAI security credentials for the OpenAI Service that you've created  \n",
        "3. Choose a model for your task  \n",
        "4. Create a simple prompt for the model  \n",
        "5. Submit your request to the model API!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Import helper libraries and instantiate credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.40.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Using cached jiter-0.5.0-cp311-none-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "Collecting sniffio (from openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai)\n",
            "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\hifazhassan\\workspace\\workshops\\azureopenai-workshop-labs-1\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
            "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.20.1 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached pydantic_core-2.20.1-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\hifazhassan\\workspace\\workshops\\azureopenai-workshop-labs-1\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Downloading openai-1.40.3-py3-none-any.whl (360 kB)\n",
            "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Using cached jiter-0.5.0-cp311-none-win_amd64.whl (191 kB)\n",
            "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "Using cached pydantic_core-2.20.1-cp311-none-win_amd64.whl (1.9 MB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: tqdm, sniffio, pydantic-core, jiter, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.7.4 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 jiter-0.5.0 openai-1.40.3 pydantic-2.8.2 pydantic-core-2.20.1 sniffio-1.3.1 tqdm-4.66.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1674829434433
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import sys\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"azure.env\")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
        "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
        "  api_version=os.getenv(\"AZURE_OPENAI_VERSION\")\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Finding the right model  \n",
        "The GPT-3.5-turbi or GPT-4 models can understand and generate natural language. The service offers four model capabilities, each with different levels of power and speed suitable for different tasks. \n",
        "\n",
        "[Azure OpenAI models](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models)  \n",
        "![](images/a-b-c-d-models-reduced.jpg)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1674742720788
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Select the General Purpose curie model for text\n",
        "model = os.getenv(\"CHAT_COMPLETION_MODEL\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Prompt Design  \n",
        "\n",
        "\"The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn concepts like\"(1):\n",
        "\n",
        "* how to spell\n",
        "* how grammar works\n",
        "* how to paraphrase\n",
        "* how to answer questions\n",
        "* how to hold a conversation\n",
        "* how to write in many languages\n",
        "* how to code\n",
        "* etc.\n",
        "\n",
        "#### How to control a large language model  \n",
        "\"Of all the inputs to a large language model, by far the most influential is the text prompt(1).\n",
        "\n",
        "Large language models can be prompted to produce output in a few ways:\n",
        "\n",
        "Instruction: Tell the model what you want\n",
        "Completion: Induce the model to complete the beginning of what you want\n",
        "Demonstration: Show the model what you want, with either:\n",
        "A few examples in the prompt\n",
        "Many hundreds or thousands of examples in a fine-tuning training dataset\"\n",
        "\n",
        "\n",
        "\n",
        "#### There are three basic guidelines to creating prompts:\n",
        "\n",
        "**Show and tell**. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
        "\n",
        "**Provide quality data**. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
        "\n",
        "**Check your settings.** The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![](images/prompt_design.jpg)\n",
        "image is creating your first text prompt!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4. Submit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1674494935186
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The use of the Oxford comma (also known as the serial comma) is a matter of style and preference, and opinions on its use can vary:\\n\\n1. **Clarity**: Proponents of the Oxford comma argue that it provides clarity and avoids potential ambiguity in sentences. For example, the sentence \"I would like to thank my parents, Oprah Winfrey, and God\" is more clear about the intended meaning than \"I would like to thank my parents, Oprah Winfrey and God,\" which could imply that Oprah and God are the parents.\\n\\n2. **Consistency**: Using the Oxford comma consistently can avoid confusion. Some style guides, like The Chicago Manual of Style and the Oxford University Press style guide, recommend its use.\\n\\n3. **Brevity**: Opponents argue that it can be unnecessary and that in many cases, the meaning is clear without it. The Associated Press (AP) Stylebook, for example, generally advises against using the Oxford comma unless it removes ambiguity.\\n\\nIn summary, whether or not to use the Oxford comma often depends on which style guide you are following or personal preference. For formal writing or where clarity is paramount, it is often safer to use the Oxford comma.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create your first prompt\n",
        "text_prompt = \"Should oxford commas always be used?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":text_prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Repeat the same call, how do the results compare?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1674494940872
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The use of Oxford commas (also known as serial commas) is a matter of style and preference rather than a strict rule, so whether they \"should\" always be used can depend on the context. \\n\\nHere are some points to consider:\\n\\n### Advantages of Using the Oxford Comma:\\n1. **Clarity**: It can help prevent ambiguity in a list. For example:\\n   - Without Oxford comma: \"I love my parents, Lady Gaga and Humpty Dumpty.\"\\n   - With Oxford comma: \"I love my parents, Lady Gaga, and Humpty Dumpty.\"\\n   The first sentence could suggest that your parents are Lady Gaga and Humpty Dumpty, while the second sentence clearly separates each item in the list.\\n\\n2. **Consistency**: Using it consistently avoids the need to decide on a case-by-case basis whether the sentence might be ambiguous without it.\\n\\n### Examples:\\n- Without Oxford comma: \"We invited the strippers, JFK and Stalin.\"\\n- With Oxford comma: \"We invited the strippers, JFK, and Stalin.\"\\n\\n### Style Guides:\\n- **APA, Chicago Manual of Style, and MLA** favor the use of the Oxford comma.\\n- **The Associated Press (AP) Stylebook** does not require it, advising that it be used only when necessary for clarity.\\n\\n### Conclusion:\\nUsing the Oxford comma can enhance clarity and consistency in your writing. While some style guides may not require it, many writers and editors prefer to use it consistently to avoid potential confusion. Ultimately, the choice can depend on your style guide, audience, or personal preference.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":text_prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Summarize Text  \n",
        "#### Challenge  \n",
        "Summarize text by adding a 'tl;dr:' to the end of a text passage. Notice how the model understands how to perform a number of tasks with no additional instructions. You can experiment with more descriptive prompts than tl;dr to modify the model’s behavior and customize the summarization you receive(3).  \n",
        "\n",
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. \n",
        "\n",
        "\n",
        "\n",
        "Tl;dr"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Exercises for several use cases  \n",
        "1. Summarize Text  \n",
        "2. Classify Text  \n",
        "3. Generate New Product Names\n",
        "4. Embeddings\n",
        "5. Fine tune a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1674495198534
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1674495201868
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Scaling up language models significantly enhances their task-agnostic, few-shot performance, enabling them to compete with state-of-the-art fine-tuning methods despite requiring far fewer examples or instructions, unlike current NLP systems.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Classify Text  \n",
        "#### Challenge  \n",
        "Classify items into categories provided at inference time. In the following example we provide both the categories and the text to classify in the prompt(*playground_reference). \n",
        "\n",
        "Customer Inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
        "\n",
        "Classified category:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1674499424645
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\n",
            "\n",
            "inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
            "\n",
            "Classified category:\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1674499378518
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Classified category: Hardware Support'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "       model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate New Product Names\n",
        "#### Challenge\n",
        "Create product names from examples words. Here we include in the prompt information about the product we are going to generate names for. We also provide a similar example to show the pattern we wish to receive. We have also set the temperature value high to increase randomness and more innovative responses.\n",
        "\n",
        "Product description: A home milkshake maker\n",
        "Seed words: fast, healthy, compact.\n",
        "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
        "\n",
        "Product description: A pair of shoes that can fit any foot size.\n",
        "Seed words: adaptable, fit, omni-fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1674257087279
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product description: A home milkshake maker\n",
            "Seed words: fast, healthy, compact.\n",
            "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
            "\n",
            "Product description: A pair of shoes that can fit any foot size.\n",
            "Seed words: adaptable, fit, omni-fit.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"### HomeShaker\\nExperience the perfect blend of speed and health with HomeShaker, your compact home milkshake maker. Designed to deliver deliciously creamy and nutritious shakes in seconds, HomeShaker is the ideal addition to any kitchen. Whether you're mixing up a protein-packed smoothie or a classic milkshake, its fast performance ensures you can enjoy your drink without the wait. With HomeShaker's user-friendly features and sleek design, making your favorite beverages has never been easier or healthier. \\n\\n### Fit Shaker\\nIntroducing Fit Shaker, the ultimate solution for creating healthy and delicious milkshakes at home. This compact and efficient milkshake maker is designed for those who value both nutrition and convenience. With Fit Shaker, you can whip up your favorite shakes in record time, thanks to its powerful motor and intuitive controls. Perfect for quick breakfasts or post-workout snacks, Fit Shaker ensures you stay on track with your health goals without sacrificing taste or time. \\n\\n### QuickShake\\nMeet QuickShake, the home milkshake maker that combines speed, health, and convenience in one compact design. QuickShake's powerful motor allows you to blend your nutritious ingredients into a smooth, delicious shake in mere seconds. Ideal for busy mornings or as a quick, healthy treat any time of day, QuickShake brings the joy of homemade shakes to your kitchen with unmatched efficiency. Enjoy a fast, easy way to sip your way to better health with QuickShake. \\n\\n### Shake Maker\\nDiscover the Shake Maker, your go-to appliance for fast, healthy, and delicious milkshakes. This compact device is perfect for any kitchen, providing the convenience you need to blend up nutritious shakes in no time. Whether you're craving a creamy dessert or a post-workout refreshment, Shake Maker's user-friendly design ensures you get the perfect consistency every time. Make healthy living simpler and more enjoyable with the Shake Maker.\\n\\n---\\n\\n### Omni-Fit Shoes\\nIntroducing Omni-Fit Shoes, the revolutionary footwear designed to adapt perfectly to any foot size. These shoes feature advanced adaptable technology, ensuring a comfortable and personalized fit for every wearer. Say goodbye to the hassle of finding the right size—Omni-Fit Shoes conform to the unique shape of your feet, providing unmatched comfort and support. Ideal for daily wear or high-activity lifestyles, Omni-Fit Shoes are the ultimate in adaptable footwear.\\n\\n### AdaptFit Footwear\\nStep into the future with AdaptFit Footwear, a groundbreaking shoe that adapts to fit any foot size seamlessly. Built with innovative omni-fit technology, these shoes offer unparalleled adaptability, molding to the contours of your feet for exceptional comfort and support. Whether you're heading to work or hitting the gym, AdaptFit Footwear ensures a perfect fit every time, making them the ideal choice for versatile and comfortable footwear.\\n\\n### Fit-All Shoes\\nSay hello to Fit-All Shoes, the versatile footwear designed to fit any foot size with ease. Thanks to their cutting-edge adaptable design, Fit-All Shoes provide a custom fit for every wearer, ensuring maximum comfort and support. Perfect for any occasion and lifestyle, these shoes eliminate the need to search for the perfect size, making them a reliable and stylish addition to your wardrobe. Experience the future of footwear with Fit-All Shoes.\\n\\n### FlexiFit Shoes\\nDiscover FlexiFit Shoes, the ultimate adaptable footwear that can fit any foot size comfortably. With state-of-the-art omni-fit technology, FlexiFit Shoes mold to the unique shape of your feet, providing excellent support and a personalized fit. These shoes are perfect for anyone looking for reliable, adaptable footwear for any activity, offering unmatched flexibility and comfort. Elevate your shoe game with the innovative and adaptable FlexiFit Shoes.\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "response = client.chat.completions.create(\n",
        "       model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt},]\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
