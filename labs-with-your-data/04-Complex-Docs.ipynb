{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
   "metadata": {},
   "source": [
    "# How to deal with complex/large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
   "metadata": {},
   "source": [
    "In the previous notebook, we developed a solution for various types of files and data formats commonly found in organizations, and this covers big majority of the use cases. However, you will find that there are issues when dealing with questions that require answers from complex files. The complexity of these files arises from their length and the way information is distributed within them. Large documents are always a challenge for Search Engines.\n",
    "\n",
    "One example of such complex files is Technical Specification Guides or Product Manuals, which can span hundreds of pages and contain information in the form of images, tables, forms, and more. Books are also complex due to their length and the presence of images or tables.\n",
    "\n",
    "These files are typically in PDF format. To better handle these PDFs, we need a smarter parsing method that treats each document as a special source and processes them page by page (1 page = 1 chunk). The objective is to obtain more accurate and faster answers from our system. Fortunately, there are usually not many of these types of documents in an organization, allowing us to make exceptions and treat them differently.\n",
    "\n",
    "If your use case is just PDFs, for example, you can just use [PyPDF library](https://pypi.org/project/pypdf/) or [Azure AI Document Intelligence SDK (former Form Recognizer)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.0.0), vectorize using OpenAI API and push the content to a vector-based index. And this is problably the simplest and fastest way to go.  However if your use case entails connecting to a datalake, or Sharepoint libraries or any other document data source with thousands of documents with multiple file types and that can change dynamically, then you would want to use the Ingestion and Document Cracking and AI-Enrichment capabilities of Azure Search engine, Notebooks 1-3, and avoid a lot of painful custom code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "from common.utils import parse_pdf, read_pdf_files, text_to_base64\n",
    "from common.prompts import DOCSEARCH_PROMPT\n",
    "from common.utils import CustomAzureSearchRetriever\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = AzureOpenAIEmbeddings(deployment=os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"], chunk_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87c647-158c-4f85-b569-5b9462f06c83",
   "metadata": {},
   "source": [
    "## 1 - Manual Document Cracking with Push to Vector-based Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551868-1546-421b-a14e-e42618d88e61",
   "metadata": {},
   "source": [
    "Within our demo storage account, we have a container named `books`, which holds 5 books of different lengths, languages, and complexities. Let's create a `cogsrch-index-books-vector` and load it with the pages of all these books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0999e24b-6a75-4fa1-9a5f-426cf0f0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FOLDER = \"./data/books/\"\n",
    "\n",
    "books = [ \"Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf\",\n",
    "         \"Fundamentals_of_Physics_Textbook.pdf\",\n",
    "         \"Made_To_Stick.pdf\",\n",
    "         \"Pere_Riche_Pere_Pauvre.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cc0db-9dae-45f2-8943-2b6fa32fcc75",
   "metadata": {},
   "source": [
    "### What to use: pyPDF or AI Documment Intelligence API (Form Recognizer)?\n",
    "\n",
    "In `utils.py` there is a **parse_pdf()** function. This utility function can parse local files using PyPDF library and can also parse local or from_url PDFs files using Azure AI Document Intelligence (Former Form Recognizer).\n",
    "\n",
    "If `form_recognizer=False`, the function will parse the PDF using the python pyPDF library, which 75% of the time does a good job.<br>\n",
    "\n",
    "Setting `form_recognizer=True`, is the best (and slower) parsing method using AI Documment Intelligence API (former known as Form Recognizer). You can specify the prebuilt model to use, the default is `model=\"prebuilt-document\"`. However, if you have a complex document with tables, charts and figures , you can try\n",
    "`model=\"prebuilt-layout\"`, and it will capture all of the nuances of each page (it takes longer of course).\n",
    "\n",
    "**Note: Many PDFs are scanned images. For example, any signed contract that was scanned and saved as PDF will NOT be parsed by pyPDF. Only AI Documment Intelligence API will work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c63a2f-7a53-4346-8a1f-483cfd159d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Text from Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf ...\n",
      "Extracting text using PyPDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 198 0 (offset 0)\n",
      "Ignoring wrong pointing object 205 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 307 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 637 0 (offset 0)\n",
      "Ignoring wrong pointing object 638 0 (offset 0)\n",
      "Ignoring wrong pointing object 640 0 (offset 0)\n",
      "Ignoring wrong pointing object 641 0 (offset 0)\n",
      "Ignoring wrong pointing object 1638 0 (offset 0)\n",
      "Ignoring wrong pointing object 1685 0 (offset 0)\n",
      "Ignoring wrong pointing object 2511 0 (offset 0)\n",
      "Ignoring wrong pointing object 2516 0 (offset 0)\n",
      "Ignoring wrong pointing object 2780 0 (offset 0)\n",
      "Ignoring wrong pointing object 2816 0 (offset 0)\n",
      "Ignoring wrong pointing object 3617 0 (offset 0)\n",
      "Ignoring wrong pointing object 3757 0 (offset 0)\n",
      "Ignoring wrong pointing object 3759 0 (offset 0)\n",
      "Ignoring wrong pointing object 3781 0 (offset 0)\n",
      "Ignoring wrong pointing object 3789 0 (offset 0)\n",
      "Ignoring wrong pointing object 6314 0 (offset 0)\n",
      "Ignoring wrong pointing object 6327 0 (offset 0)\n",
      "Ignoring wrong pointing object 6395 0 (offset 0)\n",
      "Ignoring wrong pointing object 6424 0 (offset 0)\n",
      "Ignoring wrong pointing object 6481 0 (offset 0)\n",
      "Ignoring wrong pointing object 7720 0 (offset 0)\n",
      "Ignoring wrong pointing object 7782 0 (offset 0)\n",
      "Ignoring wrong pointing object 7972 0 (offset 0)\n",
      "Ignoring wrong pointing object 7979 0 (offset 0)\n",
      "Ignoring wrong pointing object 7990 0 (offset 0)\n",
      "Ignoring wrong pointing object 7998 0 (offset 0)\n",
      "Ignoring wrong pointing object 8336 0 (offset 0)\n",
      "Ignoring wrong pointing object 8353 0 (offset 0)\n",
      "Ignoring wrong pointing object 8803 0 (offset 0)\n",
      "Ignoring wrong pointing object 8819 0 (offset 0)\n",
      "Ignoring wrong pointing object 8945 0 (offset 0)\n",
      "Ignoring wrong pointing object 8948 0 (offset 0)\n",
      "Ignoring wrong pointing object 8951 0 (offset 0)\n",
      "Ignoring wrong pointing object 8974 0 (offset 0)\n",
      "Ignoring wrong pointing object 9022 0 (offset 0)\n",
      "Ignoring wrong pointing object 9028 0 (offset 0)\n",
      "Ignoring wrong pointing object 9127 0 (offset 0)\n",
      "Ignoring wrong pointing object 9348 0 (offset 0)\n",
      "Ignoring wrong pointing object 9557 0 (offset 0)\n",
      "Ignoring wrong pointing object 9578 0 (offset 0)\n",
      "Ignoring wrong pointing object 9595 0 (offset 0)\n",
      "Ignoring wrong pointing object 9716 0 (offset 0)\n",
      "Ignoring wrong pointing object 9747 0 (offset 0)\n",
      "Ignoring wrong pointing object 10122 0 (offset 0)\n",
      "Ignoring wrong pointing object 10132 0 (offset 0)\n",
      "Ignoring wrong pointing object 10169 0 (offset 0)\n",
      "Ignoring wrong pointing object 10196 0 (offset 0)\n",
      "Ignoring wrong pointing object 10337 0 (offset 0)\n",
      "Ignoring wrong pointing object 10426 0 (offset 0)\n",
      "Ignoring wrong pointing object 10439 0 (offset 0)\n",
      "Ignoring wrong pointing object 10446 0 (offset 0)\n",
      "Ignoring wrong pointing object 10453 0 (offset 0)\n",
      "Ignoring wrong pointing object 10561 0 (offset 0)\n",
      "Ignoring wrong pointing object 10612 0 (offset 0)\n",
      "Ignoring wrong pointing object 10722 0 (offset 0)\n",
      "Ignoring wrong pointing object 10842 0 (offset 0)\n",
      "Ignoring wrong pointing object 10864 0 (offset 0)\n",
      "Ignoring wrong pointing object 11010 0 (offset 0)\n",
      "Ignoring wrong pointing object 11023 0 (offset 0)\n",
      "Ignoring wrong pointing object 11127 0 (offset 0)\n",
      "Ignoring wrong pointing object 11139 0 (offset 0)\n",
      "Ignoring wrong pointing object 11242 0 (offset 0)\n",
      "Ignoring wrong pointing object 11270 0 (offset 0)\n",
      "Ignoring wrong pointing object 11284 0 (offset 0)\n",
      "Ignoring wrong pointing object 11324 0 (offset 0)\n",
      "Ignoring wrong pointing object 11342 0 (offset 0)\n",
      "Ignoring wrong pointing object 11350 0 (offset 0)\n",
      "Ignoring wrong pointing object 11384 0 (offset 0)\n",
      "Ignoring wrong pointing object 11418 0 (offset 0)\n",
      "Ignoring wrong pointing object 11430 0 (offset 0)\n",
      "Ignoring wrong pointing object 11780 0 (offset 0)\n",
      "Ignoring wrong pointing object 11793 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing took: 1.730090 seconds\n",
      "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf contained 357 pages\n",
      "\n",
      "Extracting Text from Fundamentals_of_Physics_Textbook.pdf ...\n",
      "Extracting text using PyPDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11853 0 (offset 0)\n",
      "Ignoring wrong pointing object 11981 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing took: 162.395267 seconds\n",
      "Fundamentals_of_Physics_Textbook.pdf contained 1450 pages\n",
      "\n",
      "Extracting Text from Made_To_Stick.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 8.503152 seconds\n",
      "Made_To_Stick.pdf contained 225 pages\n",
      "\n",
      "Extracting Text from Pere_Riche_Pere_Pauvre.pdf ...\n",
      "Extracting text using PyPDF\n",
      "Parsing took: 1.430705 seconds\n",
      "Pere_Riche_Pere_Pauvre.pdf contained 225 pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_pages_map = dict()\n",
    "for book in books:\n",
    "    print(\"Extracting Text from\",book,\"...\")\n",
    "    \n",
    "    # Capture the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse the PDF\n",
    "    book_path = LOCAL_FOLDER+book\n",
    "    book_map = parse_pdf(file=book_path, form_recognizer=False, verbose=True)\n",
    "    book_pages_map[book]= book_map\n",
    "    \n",
    "    # Capture the end time and Calculate the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
    "    print(f\"{book} contained {len(book_map)} pages\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0a722-ae0c-4b57-802a-518f5d4d93fd",
   "metadata": {},
   "source": [
    "Now let's check a random page of each book to make sure the parsing was done correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a5d62f-b664-4662-a6c9-a1eb2a3c5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundaries_When_to_Say_Yes_How_to_Say_No_to_Take_Control_of_Your_Life.pdf \n",
      " chunk text: 22\n",
      "11:50 P.M.\n",
      "Lying in bed, Sherrie couldn’t tell which was greater, her lone-\n",
      "liness or her exhaustion. Deciding it was ...\n",
      "\n",
      "Fundamentals_of_Physics_Textbook.pdf \n",
      " chunk text: xx\n",
      "Tutoring problem available (at instructor’s discretion) in WileyPLUSand WebAssignSSMWorked-out solution available in  ...\n",
      "\n",
      "Made_To_Stick.pdf \n",
      " chunk text: You use flags. You tap the existing memory terrain of your audience. \n",
      "You use what's already there.  \n",
      "SIMPLE             ...\n",
      "\n",
      "Pere_Riche_Pere_Pauvre.pdf \n",
      " chunk text: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bookname,bookmap in book_pages_map.items():\n",
    "    print(bookname,\"\\n\",\"chunk text:\",bookmap[random.randint(10, 50)][2][:120],\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdc1ee-71fc-49d2-8e7c-0964bc3a4370",
   "metadata": {},
   "source": [
    "As we can see above, all books were parsed except `Pere_Riche_Pere_Pauvre.pdf` (this book is \"Rich Dad, Poor Dad\" written in French), why? Well, as we mentioned above, this book was scanned, so each page is an image and with a very unique font. We need a good PDF parser with good OCR capabilities in order to extract the content of this PDF. \n",
    "Let's try to parse this book again, but this time using Azure Document Intelligence API (former Form Recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801c6bc2-467c-4418-aa7e-ef89a1e20e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text using Azure Document Intelligence\n",
      "CPU times: total: 11.6 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "book = \"Pere_Riche_Pere_Pauvre.pdf\"\n",
    "book_path = LOCAL_FOLDER+book\n",
    "book_map = parse_pdf(file=book_path, form_recognizer=True, model=\"prebuilt-document\",from_url=False, verbose=True)\n",
    "book_pages_map[book]= book_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f9c5bb-c44b-4a4d-9780-591f9f8d128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pere_Riche_Pere_Pauvre.pdf \n",
      " chunk text: à elle seule donner les résultats voulus est que la plupart des gens ont fréquen ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(book,\"\\n\",\"chunk text:\",book_map[random.randint(10, 50)][2][:80],\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c279dfb-4fed-41b8-89e1-0ca2cefbcdc9",
   "metadata": {},
   "source": [
    "As demonstrated above, Azure Document Intelligence proves to be superior to pyPDF. **For production scenarios, we strongly recommend using Azure Document Intelligence consistently**. When doing so, it's important to make a wise choice between the available models, such as \"prebuilt-document,\" \"prebuilt-layout,\" or others. You can find more information on model selection [HERE](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature?view=doc-intel-3.0.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index\n",
    "\n",
    "\n",
    "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index_name = \"cogsrch-index-books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faab899-977b-40d0-b36e-26f75ac07e54",
   "metadata": {},
   "source": [
    "\n",
    "Please note the following points regarding the index:\n",
    "\n",
    "- The ParentKey field is absent.\n",
    "- The page_num field is present.\n",
    "\n",
    "The absence of the ParentKey field is due to the utilization of a PUSH method, rather than a PULL method. This approach indicates that we are not leveraging the integrated indexing provided by the Azure AI Search engine. Instead, we are engaging in the process of parsing, performing OCR, and manually creating and pushing the content along with its vectors.\n",
    "\n",
    "This manual parsing process involves the use of either, the pyPDF library, or the Azure Document Intelligence API. These APIs allow for the segmentation of content by page rather than by a specified number of characters, which is the method employed by the Azure AI search indexer. Consequently, this enables the inclusion of page_num as a field in our index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d63e68-69a5-4b3b-8eb0-86da02cb7230",
   "metadata": {},
   "source": [
    "REST API version 2023-10-01-Preview supports external and internal vectorization. This Notebook assumes an external vectorization strategy. This API also supports:\n",
    "    \n",
    "- vectorSearch algorithms, hnsw and exhaustiveKnn nearest neighbors, with parameters for indexing and scoring.\n",
    "- vectorProfiles for multiple combinations of algorithm configurations.\n",
    "\n",
    "Vector search algorithms include **exhaustive k-nearest neighbors (KNN)** and **Hierarchical Navigable Small World (HNSW)**. Exhaustive KNN performs a brute-force search that scans the entire vector space. HNSW performs an approximate nearest neighbor (ANN) search. While KNN provides exact nearest neighbor search results with high accuracy, its computational cost and poor scalability make it impractical for large datasets or real-time applications. HNSW, on the other hand, offers a highly efficient and scalable solution for nearest neighbor searches by finding approximate nearest neighbors quickly, making it more suitable for large-scale and high-dimensional data applications.\n",
    "\n",
    "\n",
    "check [HERE](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-create-index?tabs=config-2023-10-01-Preview%2Crest-2023-11-01%2Cpush%2Cportal-check-index) for the details of the vector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index_payload = {\n",
    "    \"name\": book_index_name,\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [  # We are showing here 3 types of search algorithms configurations that you can do\n",
    "             {\n",
    "                 \"name\": \"my-hnsw-config-1\",\n",
    "                 \"kind\": \"hnsw\",\n",
    "                 \"hnswParameters\": {\n",
    "                     \"m\": 4,\n",
    "                     \"efConstruction\": 400,\n",
    "                     \"efSearch\": 500,\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             },\n",
    "             {\n",
    "                 \"name\": \"my-hnsw-config-2\",\n",
    "                 \"kind\": \"hnsw\",\n",
    "                 \"hnswParameters\": {\n",
    "                     \"m\": 8,\n",
    "                     \"efConstruction\": 800,\n",
    "                     \"efSearch\": 800,\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             },\n",
    "             {\n",
    "                 \"name\": \"my-eknn-config\",\n",
    "                 \"kind\": \"exhaustiveKnn\",\n",
    "                 \"exhaustiveKnnParameters\": {\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             }\n",
    "        ],\n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"openai\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\":\n",
    "                {\n",
    "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                    \"apiKey\" : os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                    \"deploymentId\" : \"text-embedding-ada-002\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [  # profiles is the diferent kind of combinations of algos and vectorizers\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-1\",\n",
    "             \"algorithm\": \"my-hnsw-config-1\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            },\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-2\",\n",
    "             \"algorithm\": \"my-hnsw-config-2\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            },\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-3\",\n",
    "             \"algorithm\": \"my-eknn-config\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "        {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"dimensions\": 1536,\n",
    "            \"vectorSearchProfile\": \"my-vector-profile-3\", # we picked profile 3 to show that this index uses eKNN vs HNSW (on prior notebooks)\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36691ff0-c4c8-49d0-bfa8-3e076ece0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to debug errors\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7dda9-4725-410e-9465-54f0298fc758",
   "metadata": {},
   "source": [
    "## Upload the Document chunks and its vectors to the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e7600-7902-48d4-b199-9d9dc0a17aa0",
   "metadata": {},
   "source": [
    "The following code will iterate over each chunk of each book and use the Azure Search Rest API upload method to insert each document with its corresponding vector (using OpenAI embedding model) to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c8aa55-1b60-4057-93db-0d4a89993a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Pere_Riche_Pere_Pauvre.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [05:34<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.28 s\n",
      "Wall time: 5min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for bookname,bookmap in book_pages_map.items():\n",
    "    if(bookname == \"Pere_Riche_Pere_Pauvre.pdf\"):\n",
    "        print(\"Uploading chunks from\",bookname)\n",
    "        for page in tqdm(bookmap):\n",
    "            try:\n",
    "                page_num = page[0] + 1\n",
    "                content = page[2]\n",
    "                book_url = LOCAL_FOLDER + bookname\n",
    "                upload_payload = {\n",
    "                    \"value\": [\n",
    "                        {\n",
    "                            \"id\": text_to_base64(bookname + str(page_num)),\n",
    "                            \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
    "                            \"chunk\": content,\n",
    "                            \"chunkVector\": embedder.embed_query(content if content!=\"\" else \"-------\"),\n",
    "                            \"name\": bookname,\n",
    "                            \"location\": book_url,\n",
    "                            \"page_num\": page_num,\n",
    "                            \"@search.action\": \"upload\"\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name + \"/docs/index\",\n",
    "                                    data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "                if r.status_code != 200:\n",
    "                    print(r.status_code)\n",
    "                    print(r.text)\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\",e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"what normally rich dad do that is different from poor dad?\"\n",
    "# QUESTION = \"Tell me a summary of the book Boundaries\"\n",
    "# QUESTION = \"Dime que significa la radiacion del cuerpo negro\"\n",
    "# QUESTION = \"what is the acronym of the main point of Made to Stick book\"\n",
    "# QUESTION = \"Tell me a python example of how do I push documents with vectors to an index using the python SDK?\"\n",
    "# QUESTION = \"who won the soccer worldcup in 1994?\" # this question should have no answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b182ade-0ddd-47a1-b1eb-2cbf435c317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [book_index_name]\n",
    "k=10 # in this index k corresponds to the top pages as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d50eecb2-ce26-4127-a62b-79735b937046",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CustomAzureSearchRetriever(indexes=[book_index_name], topK=k, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2f3f2-2d66-4bd4-b90b-d30970b71af4",
   "metadata": {},
   "source": [
    "**Note**: that we are picking a larger k=10 since these chunks are NOT of 5000 chars each like prior notebooks, but instead each page is a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "410ff796-dab1-4817-a3a5-82eeff6c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 2500\n",
    "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS).configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"gpt35\",\n",
    "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0, max_tokens=COMPLETION_TOKENS),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c2851-08c5-4e7c-ba7b-4655a6021e1e",
   "metadata": {},
   "source": [
    "In `utils.py` we created the **CustomAzureSearchRetriever** class that we will use going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26f47c69-44d8-48e3-974e-7989b4a8b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | DOCSEARCH_PROMPT  # Passes the 4 variables above to the prompt template\n",
    "    | llm   # Passes the finished prompt to the LLM\n",
    "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a8761d-2c1e-4369-b7c4-c3571a0793e9",
   "metadata": {},
   "source": [
    "#### With GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14b77511-b178-4c9b-9fa5-fdddb0d3e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dans le livre \"Père riche, père pauvre\" de Robert T. Kiyosaki, plusieurs différences fondamentales sont mises en avant entre les comportements et les mentalités du \"père riche\" et du \"père pauvre\". Voici quelques-unes des principales différences :\n",
      "\n",
      "1. **Approche envers l'argent** :\n",
      "   - Le père riche croit que \"le manque d'argent est la racine de tous les maux\", tandis que le père pauvre pense que \"l'amour de l'argent est la racine de tous les maux\" [[1]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "   - Le père riche enseigne que \"les riches font en sorte que l'argent travaille pour eux\", alors que le père pauvre croit que \"les pauvres et la classe moyenne travaillent pour l'argent\" [[2]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "\n",
      "2. **Éducation et apprentissage** :\n",
      "   - Le père riche encourage l'apprentissage des rudiments de l'argent et de son fonctionnement, tandis que le père pauvre croit que \"l'argent n'est pas important et que si vous excellez dans la poursuite de votre éducation, l'argent suivra automatiquement\" [[3]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "   - Le père riche recommande d'apprendre à gérer les risques, alors que le père pauvre conseille d'éviter les risques [[4]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "\n",
      "3. **Investissements et actifs** :\n",
      "   - Le père riche considère que sa maison est un passif, tandis que le père pauvre la voit comme son plus grand actif [[5]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "   - Le père riche encourage à investir dans des actifs qui génèrent des revenus, alors que le père pauvre se concentre sur la sécurité de l'emploi et les avantages sociaux [[6]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "\n",
      "4. **Mentalité et habitudes** :\n",
      "   - Le père riche incite à poser des questions comme \"Comment puis-je me permettre d'acheter cela ?\", ce qui stimule la réflexion et la recherche de solutions, tandis que le père pauvre se contente de dire \"Je ne peux pas me permettre d'acheter cela\" [[7]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "   - Le père riche croit que \"les taxes punissent ceux qui produisent et récompensent ceux qui ne produisent pas\", alors que le père pauvre pense que \"les riches doivent payer plus de taxes pour prendre soin des êtres moins fortunés\" [[8]](./data/books/Pere_Riche_Pere_Pauvre.pdf).\n",
      "\n",
      "Ces différences montrent comment les mentalités et les approches distinctes envers l'argent et l'éducation financière peuvent influencer les résultats financiers à long terme."
     ]
    }
   ],
   "source": [
    "for chunk in chain.with_config(configurable={\"model\": \"gpt4\"}).stream(\n",
    "    {\"question\": QUESTION, \"language\": \"English\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941796c-7655-4888-a358-8a62e380bd7e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we learned how to deal with complex and large Documents and make them available for Q&A over them using [Hybrid Search](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector#hybrid-search) (text + vector search).\n",
    "\n",
    "We also learned the power of Azure Document Inteligence API and why it is recommended for production scenarios where manual Document parsing (instead of Azure Search Indexer Document Cracking) is necessary.\n",
    "\n",
    "Using Azure AI Search with its Vector capabilities and hybrid search features eliminates the need for other vector databases such as Weaviate, Qdrant, Milvus, Pinecone, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9a7d1-f029-416b-8eb2-00a8afb9151d",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "So far we have learned how to use OpenAI vectors and completion APIs in order to get an excelent answer from our documents stored in Azure AI Search. This is the backbone for a GPT Smart Search Engine.\n",
    "\n",
    "However, we are missing something: **How to have a conversation with this engine?**\n",
    "\n",
    "On the next Notebook, we are going to understand the concept of **memory**. This is necessary in order to have a chatbot that can establish a conversation with the user. Without memory, there is no real conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed733fb-dec4-4a8f-bff0-c7cbbcc0328e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
