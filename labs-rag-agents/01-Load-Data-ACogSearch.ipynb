{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to this repository. We will be walking you to a series of notebooks in which you will understand how RAG works (Retrieval Augmented Generation, a technique that combines the power of search and generation of AI to answer user queries). We will work with different sources (Azure AI Search, Files, SQL Server, Websites, APIs, etc) and at the end of the notebooks you will understand why the magic happens with the combination of:\n",
    "\n",
    "1) Multi-Agents: Agents talking to each other\n",
    "2) Azure OpenAI models\n",
    "3) Very detailed prompts\n",
    "\n",
    "But we need to start from the basics, so let's begin with Azure AI Search and how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Enrich multiple file types Azure AI Search\n",
    "\n",
    "In this Jupyter Notebook, we create and run enrichment steps to unlock searchable content in the specified Azure blob. It performs operations over mixed content in Azure Storage, such as images and application files, using a skillset that analyzes and extracts text information that becomes searchable in Azure Cognitive Search. \n",
    "The reference sample can be found at [Tutorial: Use Python and AI to generate searchable content from Azure blobs](https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob-python).\n",
    "\n",
    "In this demo we are going to be using a private (so we can mimic a private data lake scenario) Blob Storage container that has ~9.8k Computer Science publication PDFs from the Arxiv dataset.\n",
    "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
    "\n",
    "If you want to explore the dataset, go [HERE](https://console.cloud.google.com/storage/browser/arxiv-dataset/arxiv/cs/pdf?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)<br>\n",
    "Note: This dataset has been copy to a public azure blob container for this demo\n",
    "\n",
    "Although only  PDF files are used here, this can be done at a much larger scale and Azure Cognitive Search supports a range of other file formats including: Microsoft Office (DOCX/DOC, XSLX/XLS, PPTX/PPT, MSG), HTML, XML, ZIP, and plain text files (including JSON).\n",
    "Azure Search support the following sources: [Data Sources Gallery](https://learn.microsoft.com/EN-US/AZURE/search/search-data-sources-gallery)\n",
    "\n",
    "This notebook creates the following objects on your search service:\n",
    "\n",
    "+ data source\n",
    "+ search index\n",
    "+ skillset\n",
    "+ indexer\n",
    "\n",
    "This notebook calls the [Search REST APIs](https://docs.microsoft.com/rest/api/searchservice/), but you can also use the Azure.Search.Documents client library in the Azure SDK for Python to perform the same steps. See this [Python quickstart](https://docs.microsoft.com/azure/search/search-get-started-python) for details.\n",
    "\n",
    "To run this notebook, you should have already created the Azure services on README. Once you've done this, you can run all cells, but the query won't return results until the indexer is finished and the search index is loaded. \n",
    "\n",
    "We recommend running each step and making sure it completes before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cog-search](./images/Cog-Search-Enrich.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 1)) (1.33.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 3)) (0.1.8)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 4)) (0.2.4)\n",
      "Collecting langchain-experimental (from -r common\\requirements.txt (line 5))\n",
      "  Downloading langchain_experimental-0.0.62-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain_mistralai (from -r common\\requirements.txt (line 6))\n",
      "  Downloading langchain_mistralai-0.1.9-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langchain_cohere (from -r common\\requirements.txt (line 7))\n",
      "  Downloading langchain_cohere-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting langgraph (from -r common\\requirements.txt (line 8))\n",
      "  Downloading langgraph-0.1.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-cli (from -r common\\requirements.txt (line 10))\n",
      "  Downloading langchain_cli-0.0.25-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting botbuilder-integration-aiohttp>=4.14.4 (from -r common\\requirements.txt (line 11))\n",
      "  Downloading botbuilder_integration_aiohttp-4.16.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 12)) (0.7.0)\n",
      "Collecting docx2txt (from -r common\\requirements.txt (line 13))\n",
      "  Using cached docx2txt-0.8-py3-none-any.whl\n",
      "Requirement already satisfied: pillow in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 14)) (10.3.0)\n",
      "Collecting pypdf (from -r common\\requirements.txt (line 15))\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: tenacity in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 16)) (8.3.0)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 17)) (2.0.30)\n",
      "Collecting pyodbc (from -r common\\requirements.txt (line 18))\n",
      "  Downloading pyodbc-5.1.0-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting tabulate (from -r common\\requirements.txt (line 19))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting azure-cosmos (from -r common\\requirements.txt (line 20))\n",
      "  Downloading azure_cosmos-4.7.0-py3-none-any.whl.metadata (70 kB)\n",
      "     ---------------------------------------- 0.0/70.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.3/70.3 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting streamlit (from -r common\\requirements.txt (line 21))\n",
      "  Downloading streamlit-1.36.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r common\\requirements.txt (line 22)) (1.0.1)\n",
      "Collecting azure-ai-formrecognizer (from -r common\\requirements.txt (line 23))\n",
      "  Downloading azure_ai_formrecognizer-3.3.3-py3-none-any.whl.metadata (64 kB)\n",
      "     ---------------------------------------- 0.0/64.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.6/64.6 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting azure-storage-blob (from -r common\\requirements.txt (line 24))\n",
      "  Downloading azure_storage_blob-12.20.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting beautifulsoup4 (from -r common\\requirements.txt (line 25))\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pydantic==1.10.13 (from -r common\\requirements.txt (line 26))\n",
      "  Downloading pydantic-1.10.13-cp311-cp311-win_amd64.whl.metadata (150 kB)\n",
      "     ---------------------------------------- 0.0/150.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 150.9/150.9 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting fastapi (from -r common\\requirements.txt (line 27))\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langserve[all] (from -r common\\requirements.txt (line 9))\n",
      "  Downloading langserve-0.2.2-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic==1.10.13->-r common\\requirements.txt (line 26)) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai->-r common\\requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai->-r common\\requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai->-r common\\requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai->-r common\\requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai->-r common\\requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (0.1.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain->-r common\\requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community->-r common\\requirements.txt (line 4)) (0.6.7)\n",
      "Collecting langchain-community (from -r common\\requirements.txt (line 4))\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain->-r common\\requirements.txt (line 2))\n",
      "  Downloading langchain_core-0.2.12-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain (from -r common\\requirements.txt (line 2))\n",
      "  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting httpx-sse<1,>=0.3.1 (from langchain_mistralai->-r common\\requirements.txt (line 6))\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tokenizers<1,>=0.15.1 (from langchain_mistralai->-r common\\requirements.txt (line 6))\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting cohere<6.0,>=5.5.6 (from langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading cohere-5.5.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas>=1.4.3 (from langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: orjson>=2 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langserve[all]->-r common\\requirements.txt (line 9)) (3.10.3)\n",
      "Collecting pyproject-toml<0.0.11,>=0.0.10 (from langserve[all]->-r common\\requirements.txt (line 9))\n",
      "  Downloading pyproject_toml-0.0.10-py3-none-any.whl.metadata (642 bytes)\n",
      "Collecting sse-starlette<2.0.0,>=1.3.0 (from langserve[all]->-r common\\requirements.txt (line 9))\n",
      "  Downloading sse_starlette-1.8.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting gitpython<4.0.0,>=3.1.40 (from langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting libcst<2.0.0,>=1.3.1 (from langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Downloading libcst-1.4.0-cp311-cp311-win_amd64.whl.metadata (17 kB)\n",
      "Collecting tomlkit<0.13.0,>=0.12.2 (from langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<0.10.0,>=0.9.0 (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Using cached typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting uvicorn<0.24.0,>=0.23.2 (from langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting botbuilder-schema==4.16.1 (from botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading botbuilder_schema-4.16.1-py2.py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting botframework-connector==4.16.1 (from botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading botframework_connector-4.16.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting botbuilder-core==4.16.1 (from botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading botbuilder_core-4.16.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: yarl>=1.8.1 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r common\\requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r common\\requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r common\\requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r common\\requirements.txt (line 2)) (6.0.5)\n",
      "Collecting botframework-streaming==4.16.1 (from botbuilder-core==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading botframework_streaming-4.16.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting jsonpickle<1.5,>=1.2 (from botbuilder-core==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading jsonpickle-1.4.2-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting msrest==0.7.* (from botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Using cached msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting urllib3<2.0.0 (from botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.3/49.3 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyJWT>=2.4.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botframework-connector==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (2.8.0)\n",
      "Collecting msal>=1.29.0 (from botframework-connector==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Downloading msal-1.29.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: azure-core>=1.24.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (1.30.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (2024.6.2)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (0.6.1)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->-r common\\requirements.txt (line 12)) (2024.5.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy->-r common\\requirements.txt (line 17)) (3.0.3)\n",
      "Collecting altair<6,>=4.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit->-r common\\requirements.txt (line 21)) (23.2)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading protobuf-5.27.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\hifazhassan\\appdata\\roaming\\python\\python311\\site-packages (from streamlit->-r common\\requirements.txt (line 21)) (6.3.1)\n",
      "Collecting watchdog<5,>=2.1.5 (from streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading watchdog-4.0.1-py3-none-win_amd64.whl.metadata (37 kB)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-ai-formrecognizer->-r common\\requirements.txt (line 23)) (1.1.28)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from azure-storage-blob->-r common\\requirements.txt (line 24)) (42.0.8)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->-r common\\requirements.txt (line 25))\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting jinja2>=2.11.2 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading ujson-5.10.0-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r common\\requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\hifazhassan\\appdata\\roaming\\python\\python311\\site-packages (from azure-core>=1.24.0->msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hifazhassan\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit->-r common\\requirements.txt (line 21)) (0.4.6)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading boto3-1.34.142-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading fastavro-1.9.5-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7)) (2.32.0.20240602)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob->-r common\\requirements.txt (line 24)) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r common\\requirements.txt (line 4)) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r common\\requirements.txt (line 4)) (0.9.0)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of fastapi-cli to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->-r common\\requirements.txt (line 27))\n",
      "  Downloading fastapi_cli-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading fastapi_cli-0.0.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi (from -r common\\requirements.txt (line 27))\n",
      "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4.0.0,>=3.1.40->langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r common\\requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r common\\requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain->-r common\\requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hifazhassan\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.4.3->langchain_cohere->-r common\\requirements.txt (line 7)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.4.3->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.4.3->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools>=42 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]->-r common\\requirements.txt (line 9)) (65.5.0)\n",
      "Collecting wheel (from pyproject-toml<0.0.11,>=0.0.10->langserve[all]->-r common\\requirements.txt (line 9))\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain->-r common\\requirements.txt (line 2)) (3.3.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hifazhassan\\appdata\\roaming\\python\\python311\\site-packages (from rich<14,>=10.14.0->streamlit->-r common\\requirements.txt (line 21)) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15.1->langchain_mistralai->-r common\\requirements.txt (line 6))\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<0.10.0,>=0.9.0->langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.142 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading botocore-1.34.142-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->-r common\\requirements.txt (line 24)) (2.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.40->langchain-cli->-r common\\requirements.txt (line 10))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai->-r common\\requirements.txt (line 6))\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai->-r common\\requirements.txt (line 6))\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.2->fastapi->-r common\\requirements.txt (line 27))\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain->-r common\\requirements.txt (line 2)) (2.4)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Downloading rpds_py-0.19.0-cp311-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r common\\requirements.txt (line 21))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.32.0.20240521-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240403-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240402-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: pip is still looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading types_requests-2.31.0.20240310-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3 (from types-requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain_cohere->-r common\\requirements.txt (line 7))\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hifazhassan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r common\\requirements.txt (line 4)) (1.0.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest==0.7.*->botbuilder-schema==4.16.1->botbuilder-integration-aiohttp>=4.14.4->-r common\\requirements.txt (line 11))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pydantic-1.10.13-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.1 MB 25.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading langchain_experimental-0.0.62-py3-none-any.whl (202 kB)\n",
      "   ---------------------------------------- 0.0/202.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 202.7/202.7 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.8/2.2 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading langchain-0.2.7-py3-none-any.whl (983 kB)\n",
      "   ---------------------------------------- 0.0/983.6 kB ? eta -:--:--\n",
      "   --------------------------------------  983.0/983.6 kB 31.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 983.6/983.6 kB 20.7 MB/s eta 0:00:00\n",
      "Downloading langchain_mistralai-0.1.9-py3-none-any.whl (13 kB)\n",
      "Downloading langchain_cohere-0.1.9-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph-0.1.6-py3-none-any.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/90.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 90.3/90.3 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading langchain_cli-0.0.25-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.1/91.1 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading botbuilder_integration_aiohttp-4.16.1-py3-none-any.whl (19 kB)\n",
      "Downloading botbuilder_core-4.16.1-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.1/115.1 kB ? eta 0:00:00\n",
      "Downloading botbuilder_schema-4.16.1-py2.py3-none-any.whl (35 kB)\n",
      "Downloading botframework_connector-4.16.1-py2.py3-none-any.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 100.2/100.2 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading botframework_streaming-4.16.1-py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Using cached msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Downloading pyodbc-5.1.0-cp311-cp311-win_amd64.whl (68 kB)\n",
      "   ---------------------------------------- 0.0/68.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 68.7/68.7 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading azure_cosmos-4.7.0-py3-none-any.whl (252 kB)\n",
      "   ---------------------------------------- 0.0/252.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 252.1/252.1 kB 16.1 MB/s eta 0:00:00\n",
      "Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
      "   ---------------------------------------- 0.0/8.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.5/8.6 MB 47.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.8/8.6 MB 44.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.6/8.6 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.3/8.6 MB 36.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.6 MB 35.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.6/8.6 MB 34.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.6/8.6 MB 30.5 MB/s eta 0:00:00\n",
      "Downloading azure_ai_formrecognizer-3.3.3-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.4/301.4 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading azure_storage_blob-12.20.0-py3-none-any.whl (392 kB)\n",
      "   ---------------------------------------- 0.0/392.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 392.2/392.2 kB 23.9 MB/s eta 0:00:00\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.8/91.8 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "   ---------------------------------------- 0.0/857.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 857.8/857.8 kB 27.3 MB/s eta 0:00:00\n",
      "Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
      "   ---------------------------------------- 0.0/173.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 173.8/173.8 kB ? eta 0:00:00\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/207.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 207.3/207.3 kB 13.1 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.2.12-py3-none-any.whl (355 kB)\n",
      "   ---------------------------------------- 0.0/355.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 355.8/355.8 kB 21.6 MB/s eta 0:00:00\n",
      "Downloading langserve-0.2.2-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 37.5 MB/s eta 0:00:00\n",
      "Downloading libcst-1.4.0-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.0 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 32.0 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Downloading protobuf-5.27.2-cp310-abi3-win_amd64.whl (426 kB)\n",
      "   ---------------------------------------- 0.0/426.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 426.9/426.9 kB 27.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp311-cp311-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.9/25.9 MB 28.7 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 2.2/25.9 MB 27.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.7/25.9 MB 29.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.9/25.9 MB 28.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.7/25.9 MB 25.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.3/25.9 MB 23.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/25.9 MB 22.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.1/25.9 MB 22.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.9/25.9 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.5/25.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.0/25.9 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.3/25.9 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.3/25.9 MB 22.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.3/25.9 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.2/25.9 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.7/25.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.5/25.9 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.1/25.9 MB 28.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.3/25.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.5/25.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.9/25.9 MB 25.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.6/25.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 20.4 MB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.6/6.9 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.4/6.9 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.2/6.9 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 17.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.8/6.9 MB 17.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.6/6.9 MB 17.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.3/6.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.1/6.9 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.9/6.9 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 16.9 MB/s eta 0:00:00\n",
      "Downloading pyproject_toml-0.0.10-py3-none-any.whl (6.9 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 26.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 25.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.7 MB/s eta 0:00:00\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.5/59.5 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading watchdog-4.0.1-py3-none-win_amd64.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.0/83.0 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.142-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.2/139.2 kB ? eta 0:00:00\n",
      "Downloading fastavro-1.9.5-cp311-cp311-win_amd64.whl (500 kB)\n",
      "   ---------------------------------------- 0.0/500.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 500.4/500.4 kB 15.8 MB/s eta 0:00:00\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 402.6/402.6 kB 12.7 MB/s eta 0:00:00\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading jsonpickle-1.4.2-py2.py3-none-any.whl (36 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "   ---------------------------------------- 0.0/88.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 88.5/88.5 kB ? eta 0:00:00\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading msal-1.29.0-py3-none-any.whl (110 kB)\n",
      "   ---------------------------------------- 0.0/110.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 110.9/110.9 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.9/143.9 kB 8.4 MB/s eta 0:00:00\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Downloading botocore-1.34.142-py3-none-any.whl (12.4 MB)\n",
      "   ---------------------------------------- 0.0/12.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.9/12.4 MB 27.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.0/12.4 MB 25.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.1/12.4 MB 24.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.9/12.4 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.1/12.4 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.2/12.4 MB 23.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.2/12.4 MB 22.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.4 MB 22.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.4 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.4/12.4 MB 19.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "   ---------------------------------------- 0.0/177.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 177.6/177.6 kB 10.5 MB/s eta 0:00:00\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rpds_py-0.19.0-cp311-none-win_amd64.whl (211 kB)\n",
      "   ---------------------------------------- 0.0/211.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 211.5/211.5 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.7/82.7 kB ? eta 0:00:00\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: types-urllib3, pytz, docx2txt, wheel, watchdog, urllib3, tzdata, types-requests, toolz, tomlkit, toml, tabulate, soupsieve, smmap, shellingham, rpds-py, pypdf, pyodbc, pydantic, pyarrow, protobuf, parameterized, oauthlib, mdurl, MarkupSafe, libcst, jsonpickle, jmespath, httpx-sse, fsspec, filelock, fastavro, click, cachetools, blinker, uvicorn, typer, starlette, referencing, pandas, markdown-it-py, jinja2, gitdb, botocore, beautifulsoup4, s3transfer, rich, requests-oauthlib, pydeck, jsonschema-specifications, huggingface-hub, gitpython, fastapi, tokenizers, sse-starlette, msrest, msal, langchain-core, jsonschema, boto3, azure-storage-blob, azure-cosmos, pyproject-toml, langgraph, langchain_mistralai, cohere, botbuilder-schema, azure-ai-formrecognizer, altair, streamlit, langserve, langchain, botframework-connector, langchain-community, botframework-streaming, langchain-experimental, langchain-cli, botbuilder-core, langchain_cohere, botbuilder-integration-aiohttp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: types-requests\n",
      "    Found existing installation: types-requests 2.32.0.20240602\n",
      "    Uninstalling types-requests-2.32.0.20240602:\n",
      "      Successfully uninstalled types-requests-2.32.0.20240602\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.3\n",
      "    Uninstalling pydantic-2.7.3:\n",
      "      Successfully uninstalled pydantic-2.7.3\n",
      "  Attempting uninstall: msal\n",
      "    Found existing installation: msal 1.28.0\n",
      "    Uninstalling msal-1.28.0:\n",
      "      Successfully uninstalled msal-1.28.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.5\n",
      "    Uninstalling langchain-core-0.2.5:\n",
      "      Successfully uninstalled langchain-core-0.2.5\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.3\n",
      "    Uninstalling langchain-0.2.3:\n",
      "      Successfully uninstalled langchain-0.2.3\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.4\n",
      "    Uninstalling langchain-community-0.2.4:\n",
      "      Successfully uninstalled langchain-community-0.2.4\n",
      "Successfully installed MarkupSafe-2.1.5 altair-5.3.0 azure-ai-formrecognizer-3.3.3 azure-cosmos-4.7.0 azure-storage-blob-12.20.0 beautifulsoup4-4.12.3 blinker-1.8.2 botbuilder-core-4.16.1 botbuilder-integration-aiohttp-4.16.1 botbuilder-schema-4.16.1 botframework-connector-4.16.1 botframework-streaming-4.16.1 boto3-1.34.142 botocore-1.34.142 cachetools-5.3.3 click-8.1.7 cohere-5.5.8 docx2txt-0.8 fastapi-0.110.3 fastavro-1.9.5 filelock-3.15.4 fsspec-2024.6.1 gitdb-4.0.11 gitpython-3.1.43 httpx-sse-0.4.0 huggingface-hub-0.23.4 jinja2-3.1.4 jmespath-1.0.1 jsonpickle-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 langchain-0.2.7 langchain-cli-0.0.25 langchain-community-0.2.7 langchain-core-0.2.12 langchain-experimental-0.0.62 langchain_cohere-0.1.9 langchain_mistralai-0.1.9 langgraph-0.1.6 langserve-0.2.2 libcst-1.4.0 markdown-it-py-3.0.0 mdurl-0.1.2 msal-1.29.0 msrest-0.7.1 oauthlib-3.2.2 pandas-2.2.2 parameterized-0.9.0 protobuf-5.27.2 pyarrow-16.1.0 pydantic-1.10.13 pydeck-0.9.1 pyodbc-5.1.0 pypdf-4.2.0 pyproject-toml-0.0.10 pytz-2024.1 referencing-0.35.1 requests-oauthlib-2.0.0 rich-13.7.1 rpds-py-0.19.0 s3transfer-0.10.2 shellingham-1.5.4 smmap-5.0.1 soupsieve-2.5 sse-starlette-1.8.2 starlette-0.37.2 streamlit-1.36.0 tabulate-0.9.0 tokenizers-0.19.1 toml-0.10.2 tomlkit-0.12.5 toolz-0.12.1 typer-0.9.4 types-requests-2.31.0.6 types-urllib3-1.26.25.14 tzdata-2024.1 urllib3-1.26.19 uvicorn-0.23.2 watchdog-4.0.1 wheel-0.43.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script watchmedo.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tabulate.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastavro.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jsonschema.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script streamlit.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langchain-server.exe is installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts langchain-cli.exe and langchain.exe are installed in 'c:\\Users\\hifazhassan\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r common\\requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
    "BLOB_CONTAINER_NAME = \"arxivcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-datasource-files\"\n",
    "index_name = \"cogsrch-index-files\"\n",
    "skillset_name = \"cogsrch-skillset-files\"\n",
    "indexer_name = \"cogsrch-indexer-files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Source (Blob container with the Arxiv CS pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "{\"@odata.context\":\"https://cog-search-k2ozpi26tpz6e.search.windows.net/$metadata#datasources/$entity\",\"@odata.etag\":\"\\\"0x8DCA0BF563E0481\\\"\",\"name\":\"cogsrch-datasource-files\",\"description\":\"Demo files to demonstrate cognitive search capabilities.\",\"type\":\"azureblob\",\"subtype\":null,\"credentials\":{\"connectionString\":null},\"container\":{\"name\":\"arxivcs\",\"query\":null},\"dataChangeDetectionPolicy\":null,\"dataDeletionDetectionPolicy\":{\"@odata.type\":\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\"},\"encryptionKey\":null,\"identity\":null}\n"
     ]
    }
   ],
   "source": [
    "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
    "\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
    "    },\n",
    "    \"dataDeletionDetectionPolicy\" : {\n",
    "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": BLOB_CONTAINER_NAME\n",
    "    }\n",
    "}\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 201 - Successfully created\n",
    "- 204 - Succesfully overwritten\n",
    "- 40X - Authentication Error\n",
    "\n",
    "For information on Change and Delete file detection please see [HERE](https://learn.microsoft.com/en-us/azure/search/search-howto-index-changed-deleted-blobs?tabs=rest-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"@odata.context\":\"https://cog-search-k2ozpi26tpz6e.search.windows.net/$metadata#datasources/$entity\",\"@odata.etag\":\"\\\\\"0x8DCA0BF563E0481\\\\\"\",\"name\":\"cogsrch-datasource-files\",\"description\":\"Demo files to demonstrate cognitive search capabilities.\",\"type\":\"azureblob\",\"subtype\":null,\"credentials\":{\"connectionString\":null},\"container\":{\"name\":\"arxivcs\",\"query\":null},\"dataChangeDetectionPolicy\":null,\"dataDeletionDetectionPolicy\":{\"@odata.type\":\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\"},\"encryptionKey\":null,\"identity\":null}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a 403 code, probably you have a wrong endpoint or key, you can debug by uncomment this\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Azure AI Search, a search index is your searchable content, available to the search engine for indexing, full text search, vector search, hybrid search, and filtered queries. An index is defined by a schema and saved to the search service, with data import following as a second step. This content exists within your search service, apart from your primary data stores, which is necessary for the millisecond response times expected in modern search applications. Except for indexer-driven indexing scenarios, the search service never connects to or queries your source data.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below how we are creating a vector store. In Azure AI Search, a vector store has an index schema that defines vector and nonvector fields, a vector configuration for algorithms that create the embedding space, and settings on vector field definitions that are used in query requests. \n",
    "\n",
    "We are also setting a semantic ranking over a result set, promoting the most semantically relevant results to the top of the stack. You can also get semantic captions, with highlights over the most relevant terms and phrases, and semantic answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create an index\n",
    "# Queries operate over the searchable fields and filterable fields in the index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [\n",
    "            {\n",
    "                \"name\": \"myalgo\",\n",
    "                \"kind\": \"hnsw\"\n",
    "            }\n",
    "        ],\n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"openai\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\":\n",
    "                {\n",
    "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                    \"apiKey\" : os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                    \"deploymentId\" : os.environ['EMBEDDING_DEPLOYMENT_NAME']\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [\n",
    "            {\n",
    "                \"name\": \"myprofile\",\n",
    "                \"algorithm\": \"myalgo\",\n",
    "                \"vectorizer\":\"openai\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"analyzer\": \"keyword\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\",\"facetable\": \"false\"},\n",
    "        {\"name\": \"ParentKey\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
    "        {\"name\": \"title\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},   \n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"dimensions\": 1536, # IMPORTANT: Make sure these dimmensions match your embedding model name\n",
    "            \"vectorSearchProfile\": \"myprofile\",\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Search capabilities\n",
    "As you can see above in the index payload, there is a `semantic configuration`. What is that?\n",
    "\n",
    "Semantic ranker is a collection of query-related capabilities that improve the quality of an initial BM25-ranked or RRF-ranked search result for text-based queries. When you enable it on your search service, semantic ranking extends the query execution pipeline in two ways:\n",
    "\n",
    "    First, it adds secondary ranking over an initial result set that was scored using BM25 or RRF. This secondary ranking uses multi-lingual, deep learning models adapted from Microsoft Bing to promote the most semantically relevant results.\n",
    "\n",
    "    Second, it extracts and returns captions and answers in the response, which you can render on a search page to improve the user's search experience.\n",
    "\n",
    "\n",
    "For deeper explanation and limitations see [HERE](https://learn.microsoft.com/en-us/azure/search/semantic-ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Skillset - OCR, Text Splitter, AzureOpenAIEmbeddingSkill\n",
    "\n",
    "We need to create now the skillset. This is a set of steps in which we use AI Services to enrich the documents by extracting information, applying OCR, splitting, and embedding chunks, among other skills.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-predefined-skills\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below that we are using IndexProjections. By default, one document processed within a skillset maps to a single document in the search index. This means that if you perform chunking of an input text and then perform enrichments on each chunk, the result in the index when mapped via outputFieldMappings is an array of the generated enrichments. **With index projections, you define a context at which to map each chunk of enriched data to its own search document**. This allows you to apply a one-to-many mapping of a document's enriched data to the search index.\n",
    "    \n",
    "The parameter: `\"projectionMode\": \"skipIndexingParentDocuments\"` allows us to skip the indexing of the parent documents, and keep only the index with the chunks and its vectors.\n",
    "\n",
    "### Content Lifecycle\n",
    "If the indexer data source supports change tracking and deletion detection, the indexing process can synchronize the primary (parend documents) and secondary indexes (chunks) to pick up those changes.\n",
    "\n",
    "Each time you run the indexer and skillset, the index projections are updated if the skillset or underlying source data has changed. Any changes picked up by the indexer are propagated through the enrichment process to the projections in the index, ensuring that your projected data is a current representation of content in the originating data source. This will save you weeks of programming and a lot of headaches trying to keep the content in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a skillset\n",
    "skillset_payload = {\n",
    "    \"name\": skillset_name,\n",
    "    \"description\": \"e2e Skillset for RAG - Files\",\n",
    "    \"skills\":\n",
    "    [\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "            \"description\": \"Extract text (plain and structured) from image.\",\n",
    "            \"context\": \"/document/normalized_images/*\",\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"detectOrientation\": True,\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                  \"name\": \"image\",\n",
    "                  \"source\": \"/document/normalized_images/*\"\n",
    "                }\n",
    "            ],\n",
    "                \"outputs\": [\n",
    "                {\n",
    "                  \"name\": \"text\",\n",
    "                  \"targetName\" : \"images_text\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "            \"description\": \"Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field. This is useful for PDF and other file formats that supported embedded images.\",\n",
    "            \"context\": \"/document\",\n",
    "            \"insertPreTag\": \" \",\n",
    "            \"insertPostTag\": \" \",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                  \"name\":\"text\", \"source\": \"/document/content\"\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"itemsToInsert\", \"source\": \"/document/normalized_images/*/images_text\"\n",
    "                },\n",
    "                {\n",
    "                  \"name\":\"offsets\", \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                  \"name\": \"mergedText\", \n",
    "                  \"targetName\" : \"merged_text\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "            \"context\": \"/document\",\n",
    "            \"textSplitMode\": \"pages\",  # although it says \"pages\" it actally means chunks, not actual pages\n",
    "            \"maximumPageLength\": 5000, # 5000 characters is default and a good choice\n",
    "            \"pageOverlapLength\": 750,  # 15% overlap among chunks\n",
    "            \"defaultLanguageCode\": \"en\",\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/merged_text\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"textItems\",\n",
    "                    \"targetName\": \"chunks\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "            \"description\": \"Azure OpenAI Embedding Skill\",\n",
    "            \"context\": \"/document/chunks/*\",\n",
    "            \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "            \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
    "            \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"source\": \"/document/chunks/*\"\n",
    "                }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"embedding\",\n",
    "                    \"targetName\": \"vector\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"indexProjections\": {\n",
    "        \"selectors\": [\n",
    "            {\n",
    "                \"targetIndexName\": index_name,\n",
    "                \"parentKeyFieldName\": \"ParentKey\",\n",
    "                \"sourceContext\": \"/document/chunks/*\",\n",
    "                \"mappings\": [\n",
    "                    {\n",
    "                        \"name\": \"title\",\n",
    "                        \"source\": \"/document/title\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"name\",\n",
    "                        \"source\": \"/document/name\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"location\",\n",
    "                        \"source\": \"/document/location\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"chunk\",\n",
    "                        \"source\": \"/document/chunks/*\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"chunkVector\",\n",
    "                        \"source\": \"/document/chunks/*/vector\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"projectionMode\": \"skipIndexingParentDocuments\"\n",
    "        }\n",
    "    },\n",
    "    \"cognitiveServices\": {\n",
    "        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "        \"description\": os.environ['COG_SERVICES_NAME'],\n",
    "        \"key\": os.environ['COG_SERVICES_KEY']\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Indexer - (runs the pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three components you have created thus far (data source, skillset, index) are inputs to an indexer. Creating the indexer on Azure Cognitive Search is the event that puts the entire pipeline into motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer\n",
    "indexer_payload = {\n",
    "    \"name\": indexer_name,\n",
    "    \"dataSourceName\": datasource_name,\n",
    "    \"targetIndexName\": index_name,\n",
    "    \"skillsetName\": skillset_name,\n",
    "    \"schedule\" : { \"interval\" : \"PT30M\"}, # How often do you want to check for new content in the data source\n",
    "    \"fieldMappings\": [\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_title\",\n",
    "          \"targetFieldName\" : \"title\"\n",
    "        },\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_storage_name\",\n",
    "          \"targetFieldName\" : \"name\"\n",
    "        },\n",
    "        {\n",
    "          \"sourceFieldName\" : \"metadata_storage_path\",\n",
    "          \"targetFieldName\" : \"location\"\n",
    "        }\n",
    "    ],\n",
    "    \"outputFieldMappings\":[],\n",
    "    \"parameters\":\n",
    "    {\n",
    "        \"maxFailedItems\": -1,\n",
    "        \"maxFailedItemsPerBatch\": -1,\n",
    "        \"configuration\":\n",
    "        {\n",
    "            \"dataToExtract\": \"contentAndMetadata\",\n",
    "            \"imageAction\": \"generateNormalizedImages\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"@odata.context\":\"https://cog-search-zf4fwhz3gdn64.search.windows.net/$metadata#indexers/$entity\",\"@odata.etag\":\"\\\\\"0x8DC7052637CA2AC\\\\\"\",\"name\":\"cogsrch-indexer-files-2\",\"description\":null,\"dataSourceName\":\"cogsrch-datasource-files-2\",\"skillsetName\":\"cogsrch-skillset-files-2\",\"targetIndexName\":\"cogsrch-index-files-2\",\"disabled\":null,\"schedule\":{\"interval\":\"PT30M\",\"startTime\":\"2024-05-09T18:03:38.4035486Z\"},\"parameters\":{\"batchSize\":null,\"maxFailedItems\":-1,\"maxFailedItemsPerBatch\":-1,\"base64EncodeKeys\":null,\"configuration\":{\"dataToExtract\":\"contentAndMetadata\",\"imageAction\":\"generateNormalizedImages\"}},\"fieldMappings\":[{\"sourceFieldName\":\"metadata_title\",\"targetFieldName\":\"title\",\"mappingFunction\":null},{\"sourceFieldName\":\"metadata_storage_name\",\"targetFieldName\":\"name\",\"mappingFunction\":null},{\"sourceFieldName\":\"metadata_storage_path\",\"targetFieldName\":\"location\",\"mappingFunction\":null}],\"outputFieldMappings\":[],\"cache\":null,\"encryptionKey\":null}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment if you find an error\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you get a 400 unauthorize error, make sure that you are using the Azure Search MANAGEMENT KEY, not the QUERY key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Status: inProgress\n",
      "Items Processed: 40\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Optionally, get indexer status to confirm that it's running\n",
    "try:\n",
    "    r = requests.get(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name +\n",
    "                     \"/status\", headers=headers, params=params)\n",
    "    # pprint(json.dumps(r.json(), indent=1))\n",
    "    print(r.status_code)\n",
    "    print(\"Status:\",r.json().get('lastResult').get('status'))\n",
    "    print(\"Items Processed:\",r.json().get('lastResult').get('itemsProcessed'))\n",
    "    print(r.ok)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Wait a few seconds until the process starts and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When the indexer finishes running we will have all documents indexed in your Search Engine!.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://learn.microsoft.com/en-us/azure/search/cognitive-search-tutorial-blob\n",
    "- https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search\n",
    "- https://learn.microsoft.com/en-us/azure/search/search-get-started-vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "In the next notebook 02, we will implement another type of indexing call One-to-Many, in which a single CSV or JSON file can be converted into multiple individual searchable documents in Azure Search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
